{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76dc1956",
   "metadata": {},
   "source": [
    "## Automatic Differentiation in PyTorch\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "559112aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.4758, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1213, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8149, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0647, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6200, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3534, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1952, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1037, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0526, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0253, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Build a linear layer with a dummy MSE loss. \n",
    "import torch\n",
    "\n",
    "dummy_input = torch.rand(16, )\n",
    "dummy_gt = torch.randn(10,)\n",
    "\n",
    "'''\n",
    "Remember to set the required_grad on the tensors that hold the weights to be optimized.\n",
    "This is akin to calling .watch() on that in Tensorflow GradientTape. \n",
    "'''\n",
    "\n",
    "w = torch.randn(10, 16, requires_grad=True)\n",
    "b = torch.randn(10, requires_grad=True)\n",
    "\n",
    "#Getting ahead of the tutorial here a little bit. PyTorch memories are getting back to me.\n",
    "optimizer = torch.optim.Adam(params=[w,b])\n",
    "\n",
    "#A trivial training loop here for 1000 epochs. Gets the job done though.\n",
    "for i in range(1000):\n",
    "    z = (w @ dummy_input) + b #feedforward.\n",
    "    loss = torch.nn.functional.mse_loss(dummy_gt, z) # compute the loss.\n",
    "    if i % 100 == 0:\n",
    "        print(loss)\n",
    "        \n",
    "    optimizer.zero_grad() #make sure to refresh the grads on all tensors that 'requires_grad'\n",
    "    '''\n",
    "    #computes the gradients and backprops them \n",
    "    to all the 'requires_grad' tensors that went into computing the 'loss'. \n",
    "    '''\n",
    "    loss.backward() \n",
    "\n",
    "    optimizer.step() #optimizer applies the gradients to the tensors it was defined it (w,b) here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a767c0a",
   "metadata": {},
   "source": [
    "### Getting the gradient function and the gradient values.:\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Note that actual gradient value is accessible only in the leaf nodes of the computational graph. I don't quite understand why though. My hypothesis: Perhaps some memory constraint issue. Once the step is taken on the weights in the middle of the network, the gradient may be deleted unless retain_graph is called.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75b1d84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z: <AddBackward0 object at 0x7f9ba13ea9d0>\n",
      "Gradient function for loss <MseLossBackward0 object at 0x7f9ba13ea520>\n"
     ]
    }
   ],
   "source": [
    "#Printing the gradient function.\n",
    "\n",
    "print('Gradient function for z:', z.grad_fn)\n",
    "print('Gradient function for loss', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1d73eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch gradient on w: torch.Size([10, 16])\n",
      "Last epoch gradient on b torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "#Print the last gradients that accumulated on the weights and biases. \n",
    "print('Last epoch gradient on w:', w.grad.shape)\n",
    "print('Last epoch gradient on b', b.grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114db11",
   "metadata": {},
   "source": [
    "### One can backprop on a computational graph only once. If you try to call .backward() on the same CG object twice, you get the error below.\n",
    "&nbsp;\n",
    "\n",
    "**If it is absolutely necessary to backprop the same CG multiple times, then use loss.backward(retain_graph=True). That saves the graph in RAM, which is costly and must be avoided unless needed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7c3f89e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23213/2859123600.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1530e090",
   "metadata": {},
   "source": [
    "## Stopping Gradient Tracking on Tensors\n",
    "\n",
    "### Why do this at all? \n",
    "&nbsp;\n",
    "* Freezing some weights of the model while fine-tuning. \n",
    "* Running the model on inference mode after training. \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Method 1: Use a context torch.no_grad() context as shown below.** \n",
    "\n",
    "**Method 2: Call the detach() method on the tensor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5420cc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "temp = torch.matmul(w, dummy_input) + b \n",
    "print(temp.requires_grad)\n",
    "\n",
    "'''\n",
    "Make sure to surrouding non-training operations on trainable tensors with no_grad() context.\n",
    "'''\n",
    "with torch.no_grad():\n",
    "    temp = torch.matmul(w, dummy_input) + b\n",
    "\n",
    "#Notice that gradients were not computed for this tensor.\n",
    "print(temp.requires_grad)\n",
    "\n",
    "#We can start gradient tracking again on this tensor.\n",
    "temp.requires_grad_(True) # Notice that this is an in_place operations as its name suggests.\n",
    "print(temp.requires_grad)\n",
    "\n",
    "'''\n",
    "Alternatively, use the .detach() method on the tensor to stop gradient tracking. \n",
    "'''\n",
    "temp.detach_()\n",
    "print(temp.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872dd11c",
   "metadata": {},
   "source": [
    "## Important Note on the DAGs in PyTorch\n",
    "&nbsp;\n",
    "\n",
    "**DAGs are dynamic in PyTorch An important thing to note is that the graph is recreated from scratch; after each .backward() call, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model; you can change the shape, size and operations at every iteration if needed.**\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**This means that when I compute loss = ... in each epoch, a new DAG is being created. This also explains why one can't call .backward() twice on the same DAG without setting retain_graph to true.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d5229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_env] *",
   "language": "python",
   "name": "conda-env-torch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
