{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab6a5cba",
   "metadata": {},
   "source": [
    "### TorchScript and Torch.JIT': Optimizing Inference Time on Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea6ae13",
   "metadata": {},
   "source": [
    "**TorchScript takes in an eager-mode PyTorch module and compiles its code into TorchScript. This format can run on non-Python environments by itself (including C++). Torch.jit(just-in-time) compiler used for generating this code optimizes the model through layer-fusion, quantization and sparsification etc. It is not meant for training, rather, use it for converting your trained model into an optimized equivalent that performs faster for inference.** \n",
    "\n",
    "&nbsp;\n",
    "Here is a good TowardsDataScience articles on this:\n",
    "\n",
    "&nbsp; \n",
    "https://towardsdatascience.com/pytorch-jit-and-torchscript-c2a77bac0fff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets, transforms\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "import time\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(LeNet5, self).__init__(**kwargs)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=(4,4))\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_features=480, out_features=84)\n",
    "        self.fc2 = nn.Linear(in_features=84, out_features=10)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = F.max_pool2d(torch.tanh(self.conv1(X)), kernel_size =(2,2))\n",
    "        h = F.max_pool2d(torch.tanh(self.conv2(h)), kernel_size =(2,2))\n",
    "        h = self.flatten(torch.tanh(self.conv3(h)))\n",
    "        h = torch.tanh(self.fc1(h))\n",
    "        h = self.fc2(h)\n",
    "        return h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4771e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d705ba15e954aeb91a76a226b6d71fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00551d0554e247f1bbe3b884e6dad307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d432fcd69d34f8d85cda4365775fd07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7c68e54b214dc2b27ba64361ca861c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78300b1ba6ef4183aed25176ac0c1e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f7e354bd954812a6dc22f8e31b9e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678eab7e86fc4ae89bd7761229cbf0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e53501c7e74ad9b3d6eeac18001478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data pipeline.\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root = 'data/MNIST',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    "\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='mnist_data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle = True,\n",
    "    drop_last=True,\n",
    "    num_workers=8\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    num_workers=8\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0bd18021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop.\n",
    "def train(model, epochs):  \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        train_loss, tr_correct_preds = 0, 0\n",
    "        val_loss, tst_correct_preds = 0, 0\n",
    "\n",
    "        for (train_X, train_y) in train_dataloader:\n",
    "            train_X, train_y = train_X.to(device), train_y.to(device)\n",
    "            train_preds = lenet5(train_X)\n",
    "            loss = loss_fn(train_preds, train_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            tr_correct_preds += (train_preds.argmax(dim=1) == train_y).sum().item()\n",
    "\n",
    "        for (val_X, val_y) in test_dataloader:\n",
    "            with torch.no_grad():\n",
    "                val_X, val_y = val_X.to(device), val_y.to(device)\n",
    "                val_preds = lenet5(val_X)\n",
    "                val_loss += loss_fn(val_preds, val_y).item()\n",
    "                tst_correct_preds += (val_preds.argmax(dim=1) == val_y).sum().item()\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        num_train_steps = len(train_dataloader.dataset) // batch_size\n",
    "        num_val_steps = len(test_dataloader.dataset) // batch_size\n",
    "        \n",
    "        print('Epoch {}: \\nTrain Loss:{}, Train acc: {}, Val loss: {}, Val acc:{},\\n'\n",
    "              'Correct Training Samples: {}, Correct Validation Samples: {} \\nEpoch took: {} seconds.\\n'\n",
    "              .format(epoch + 1,\n",
    "                      train_loss/num_train_steps,\n",
    "                      tr_correct_preds/ (num_train_steps * batch_size),\n",
    "                      val_loss/num_val_steps,\n",
    "                      tst_correct_preds/ (num_val_steps * batch_size),\n",
    "                      tr_correct_preds, tst_correct_preds,\n",
    "                      (end - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92871714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters and other configuration.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "optimizer = torch.optim.SGD(lenet5.parameters(), lr=learning_rate,\n",
    "                            momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b04de65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: \n",
      "Train Loss:0.011836165772801168, Train acc: 0.9974319637139808, Val loss: 0.030782481208053724, Val acc:0.9896834935897436,\n",
      "Correct Training Samples: 59814, Correct Validation Samples: 9881 \n",
      "Epoch took: 4.035372495651245 seconds.\n",
      "\n",
      "Epoch 2: \n",
      "Train Loss:0.011740395764460223, Train acc: 0.9975653681963714, Val loss: 0.030068295101675455, Val acc:0.9895833333333334,\n",
      "Correct Training Samples: 59822, Correct Validation Samples: 9880 \n",
      "Epoch took: 3.941103935241699 seconds.\n",
      "\n",
      "Epoch 3: \n",
      "Train Loss:0.01139823569482085, Train acc: 0.9976654215581644, Val loss: 0.030282602170000946, Val acc:0.9893830128205128,\n",
      "Correct Training Samples: 59828, Correct Validation Samples: 9878 \n",
      "Epoch took: 4.0197389125823975 seconds.\n",
      "\n",
      "Epoch 4: \n",
      "Train Loss:0.011241211394320681, Train acc: 0.9976987726787621, Val loss: 0.03065123837908546, Val acc:0.9894831730769231,\n",
      "Correct Training Samples: 59830, Correct Validation Samples: 9879 \n",
      "Epoch took: 3.904510498046875 seconds.\n",
      "\n",
      "Epoch 5: \n",
      "Train Loss:0.01104152475728722, Train acc: 0.9975320170757738, Val loss: 0.029476469322710332, Val acc:0.9902844551282052,\n",
      "Correct Training Samples: 59820, Correct Validation Samples: 9887 \n",
      "Epoch took: 3.8311588764190674 seconds.\n",
      "\n",
      "Epoch 6: \n",
      "Train Loss:0.010905786026495319, Train acc: 0.9978488527214514, Val loss: 0.029926452146425548, Val acc:0.9898838141025641,\n",
      "Correct Training Samples: 59839, Correct Validation Samples: 9883 \n",
      "Epoch took: 3.918314218521118 seconds.\n",
      "\n",
      "Epoch 7: \n",
      "Train Loss:0.010644675642066847, Train acc: 0.9977821504802561, Val loss: 0.029241341408123844, Val acc:0.9902844551282052,\n",
      "Correct Training Samples: 59835, Correct Validation Samples: 9887 \n",
      "Epoch took: 3.905660629272461 seconds.\n",
      "\n",
      "Epoch 8: \n",
      "Train Loss:0.010480601054525824, Train acc: 0.9979655816435432, Val loss: 0.029203849005240885, Val acc:0.9896834935897436,\n",
      "Correct Training Samples: 59846, Correct Validation Samples: 9881 \n",
      "Epoch took: 3.8782074451446533 seconds.\n",
      "\n",
      "Epoch 9: \n",
      "Train Loss:0.010290918271047368, Train acc: 0.9979322305229456, Val loss: 0.030179342204064596, Val acc:0.9898838141025641,\n",
      "Correct Training Samples: 59844, Correct Validation Samples: 9883 \n",
      "Epoch took: 3.883875846862793 seconds.\n",
      "\n",
      "Epoch 10: \n",
      "Train Loss:0.010102655457617495, Train acc: 0.9979989327641409, Val loss: 0.03085282723305938, Val acc:0.9889823717948718,\n",
      "Correct Training Samples: 59848, Correct Validation Samples: 9874 \n",
      "Epoch took: 3.874307870864868 seconds.\n",
      "\n",
      "Epoch 11: \n",
      "Train Loss:0.009956911058179297, Train acc: 0.9980156083244397, Val loss: 0.030289100796952156, Val acc:0.9893830128205128,\n",
      "Correct Training Samples: 59849, Correct Validation Samples: 9878 \n",
      "Epoch took: 3.838585615158081 seconds.\n",
      "\n",
      "Epoch 12: \n",
      "Train Loss:0.009754330580288757, Train acc: 0.9980156083244397, Val loss: 0.030325650179059804, Val acc:0.9897836538461539,\n",
      "Correct Training Samples: 59849, Correct Validation Samples: 9882 \n",
      "Epoch took: 3.9417717456817627 seconds.\n",
      "\n",
      "Epoch 13: \n",
      "Train Loss:0.00964126576575214, Train acc: 0.998082310565635, Val loss: 0.029248586725467856, Val acc:0.9898838141025641,\n",
      "Correct Training Samples: 59853, Correct Validation Samples: 9883 \n",
      "Epoch took: 3.92749285697937 seconds.\n",
      "\n",
      "Epoch 14: \n",
      "Train Loss:0.009444467819486784, Train acc: 0.9982157150480256, Val loss: 0.030325377323466594, Val acc:0.9893830128205128,\n",
      "Correct Training Samples: 59861, Correct Validation Samples: 9878 \n",
      "Epoch took: 3.801877498626709 seconds.\n",
      "\n",
      "Epoch 15: \n",
      "Train Loss:0.009284300170167922, Train acc: 0.998082310565635, Val loss: 0.030528083954870968, Val acc:0.9898838141025641,\n",
      "Correct Training Samples: 59853, Correct Validation Samples: 9883 \n",
      "Epoch took: 3.8439199924468994 seconds.\n",
      "\n",
      "Epoch 16: \n",
      "Train Loss:0.009079841142242005, Train acc: 0.998282417289221, Val loss: 0.029643079049212526, Val acc:0.9899839743589743,\n",
      "Correct Training Samples: 59865, Correct Validation Samples: 9884 \n",
      "Epoch took: 3.9099326133728027 seconds.\n",
      "\n",
      "Epoch 17: \n",
      "Train Loss:0.009003815045444844, Train acc: 0.9983324439701174, Val loss: 0.029911273879080023, Val acc:0.9903846153846154,\n",
      "Correct Training Samples: 59868, Correct Validation Samples: 9888 \n",
      "Epoch took: 3.894425630569458 seconds.\n",
      "\n",
      "Epoch 18: \n",
      "Train Loss:0.00883486594064529, Train acc: 0.9983491195304163, Val loss: 0.029889539414468275, Val acc:0.9902844551282052,\n",
      "Correct Training Samples: 59869, Correct Validation Samples: 9887 \n",
      "Epoch took: 3.89021635055542 seconds.\n",
      "\n",
      "Epoch 19: \n",
      "Train Loss:0.00861760425383723, Train acc: 0.9985325506937033, Val loss: 0.031668170645086015, Val acc:0.9884815705128205,\n",
      "Correct Training Samples: 59880, Correct Validation Samples: 9869 \n",
      "Epoch took: 3.828603744506836 seconds.\n",
      "\n",
      "Epoch 20: \n",
      "Train Loss:0.008475678461206742, Train acc: 0.9985492262540021, Val loss: 0.02989694679131506, Val acc:0.9902844551282052,\n",
      "Correct Training Samples: 59881, Correct Validation Samples: 9887 \n",
      "Epoch took: 3.8687527179718018 seconds.\n",
      "\n",
      "Epoch 21: \n",
      "Train Loss:0.00840725534198097, Train acc: 0.9983324439701174, Val loss: 0.029464211507076517, Val acc:0.9900841346153846,\n",
      "Correct Training Samples: 59868, Correct Validation Samples: 9885 \n",
      "Epoch took: 3.828947067260742 seconds.\n",
      "\n",
      "Epoch 22: \n",
      "Train Loss:0.00825128118974592, Train acc: 0.9984825240128068, Val loss: 0.030058244058520173, Val acc:0.9899839743589743,\n",
      "Correct Training Samples: 59877, Correct Validation Samples: 9884 \n",
      "Epoch took: 3.935072660446167 seconds.\n",
      "\n",
      "Epoch 23: \n",
      "Train Loss:0.008104895863749993, Train acc: 0.9986492796157951, Val loss: 0.030432673698109845, Val acc:0.9899839743589743,\n",
      "Correct Training Samples: 59887, Correct Validation Samples: 9884 \n",
      "Epoch took: 3.9298648834228516 seconds.\n",
      "\n",
      "Epoch 24: \n",
      "Train Loss:0.007955715239360847, Train acc: 0.9985992529348986, Val loss: 0.02982330183005662, Val acc:0.9892828525641025,\n",
      "Correct Training Samples: 59884, Correct Validation Samples: 9877 \n",
      "Epoch took: 3.8773109912872314 seconds.\n",
      "\n",
      "Epoch 25: \n",
      "Train Loss:0.007764337217348147, Train acc: 0.9986993062966916, Val loss: 0.02990451819804991, Val acc:0.9900841346153846,\n",
      "Correct Training Samples: 59890, Correct Validation Samples: 9885 \n",
      "Epoch took: 3.883781671524048 seconds.\n",
      "\n",
      "Epoch 26: \n",
      "Train Loss:0.007696687907660494, Train acc: 0.9987159818569904, Val loss: 0.02959131554745489, Val acc:0.9903846153846154,\n",
      "Correct Training Samples: 59891, Correct Validation Samples: 9888 \n",
      "Epoch took: 3.7923569679260254 seconds.\n",
      "\n",
      "Epoch 27: \n",
      "Train Loss:0.007495870264264454, Train acc: 0.9987826840981857, Val loss: 0.029497257521982716, Val acc:0.9901842948717948,\n",
      "Correct Training Samples: 59895, Correct Validation Samples: 9886 \n",
      "Epoch took: 3.905412197113037 seconds.\n",
      "\n",
      "Epoch 28: \n",
      "Train Loss:0.007509745733325172, Train acc: 0.9987660085378869, Val loss: 0.030097654543472647, Val acc:0.9901842948717948,\n",
      "Correct Training Samples: 59894, Correct Validation Samples: 9886 \n",
      "Epoch took: 3.885435104370117 seconds.\n",
      "\n",
      "Epoch 29: \n",
      "Train Loss:0.0073475793800493175, Train acc: 0.9987159818569904, Val loss: 0.030398671721787906, Val acc:0.989082532051282,\n",
      "Correct Training Samples: 59891, Correct Validation Samples: 9875 \n",
      "Epoch took: 3.91679048538208 seconds.\n",
      "\n",
      "Epoch 30: \n",
      "Train Loss:0.007164763260028685, Train acc: 0.998849386339381, Val loss: 0.03041952818402467, Val acc:0.9899839743589743,\n",
      "Correct Training Samples: 59899, Correct Validation Samples: 9884 \n",
      "Epoch took: 3.8995893001556396 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialize the dynamic model. \n",
    "lenet5_dynamic = LeNet5(10).to(device)\n",
    "train(lenet5_dynamic, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad46a12e",
   "metadata": {},
   "source": [
    "### Once the model is trained, one can compile it with JIT and save it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a638538e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.jit._script.RecursiveScriptModule'>\n"
     ]
    }
   ],
   "source": [
    "lenet5_optimized = torch.jit.script(lenet5_dynamic) # this is way 1, does not require dummy input.\n",
    "\n",
    "\n",
    "'''\n",
    "Another alternative is to run torch.jit.trace(model, dummy_input_tensor)\n",
    "Trace does the same thing, but .script seems to be more robust to control flow and type handling. \n",
    "Also .script() does not require dummy input. \n",
    "If you use .trace() make sure to send the dummy_input tensor to the same device as the model.\n",
    "'''\n",
    "\n",
    "print(type(lenet5_optimized))\n",
    "#save the optimized script into .pth file. \n",
    "lenet5_optimized.save('lenet5_compute_graph.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d153bdb",
   "metadata": {},
   "source": [
    "### Then one can load the model back on from .pth file like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "34c6f901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.jit._script.RecursiveScriptModule'>\n"
     ]
    }
   ],
   "source": [
    "lenet5_loaded = torch.jit.load('lenet5_compute_graph.pth')\n",
    "lenet5_loaded = lenet5_loaded.to(device) # benchmark on GPU.\n",
    "print(type(lenet5_loaded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f63bea",
   "metadata": {},
   "source": [
    "### Benchmarking inference speed of the eager mode and static mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "66c6ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_time_benchmark(model, runs, batch_size):\n",
    "    input_batch = torch.rand(size=[batch_size, 1, 28, 28]).cuda() #dummy MNIST batch for benchmarking.\n",
    "    total_time = 0\n",
    "    start = time.time()\n",
    "    for i in range(runs):\n",
    "        _ = model(input_batch)\n",
    "        total_time += time.time() - start\n",
    "    \n",
    "    return total_time / runs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "83e2f905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9165674172903597\n"
     ]
    }
   ],
   "source": [
    "dynamic_time = inference_time_benchmark(lenet5_dynamic, 10000, 64)\n",
    "static_time = inference_time_benchmark(lenet5_loaded, 10000, 64)\n",
    "\n",
    "print(static_time / dynamic_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eeba9b",
   "metadata": {},
   "source": [
    "### Voila! 8.4% speedup over eager mode. Remember this when deploying a trained model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_env] *",
   "language": "python",
   "name": "conda-env-torch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
